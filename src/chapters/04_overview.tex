\chapter{Overview of the Approach}
\label{chapter:overview}

This chapter gives an overview of my approach of JavaScript static analysis using \emph{Codemodel-Rifle}, and describes all — modularity- and performance-related — \emph{architectural} changes of the framework.


\section{Rearchitecturing the Codemodel-Rifle Framework}

Dániel Stein, creator of the Codemodel-Rifle framework, details the design of the framework in his Master's Thesis~\cite{stein-daniel-msc}. Following his thesis and my experiences with the framework, \Cref{fig:codemodel-rifle-original-architecture} and the below specification summarises the software's original architecture:

\begin{itemize}
\item A source code file is delivered to Codemodel-Rifle via the HTTP REST API of the framework's embedded webserver as a text.
\item The framework parses the incoming source file into an AST model with Shape Security's Shift parser.
\item The framework performs scope analysis on the AST model with Shape Security's scope analyser, transforming the AST model into an ASG model.
\item The ASG model is transformed to a property graph and is stored in the framework's embedded Neo4j graph database.
\item Apart from importing a file, the framework is able to perform analyses on or visualisation of a graph stored in its database, if requested over its REST API.
\item Analysing more than one \es modules coherently is only minimally supported: interconnecting the related modules' subgraphs along the \emph{export} and \emph{import} \es statements is implemented for one use case only, out of more than 80.
\item The result of the analyses or the visualisation is returned via the REST API in JSON or in a visual file format.
\end{itemize}

Codemodel-Rifle was notably refactored since then. This section introduces why refactoring was necessary, and presents the details and the results of the process.

\begin{figure}[!p]
	\centering
	\includegraphics[height=\textheight-18mm,clip]{figures/codemodel-rifle-original-architecture.pdf}
	\caption{The original architecture of the Codemodel-Rifle framework}
	\label{fig:codemodel-rifle-original-architecture}
\end{figure}


\subsection{Open-Sourcing and Licensing Issues}

The development of the Codemodel-Rifle framework was supported by the Fault-Tolerant Systems Research Group (FTSRG) of the Budapest University of Technology and Economics. FTSRG's decision — with the support of Dániel Stein — was to open-source the framework under the Eclipse Public License, version 1.0 (EPLv1)~\cite{eplv1}. This introduced licensing problems as follows.

The framework uses Neo4j as its internal graph data storage, and Neo4j was embedded into Codemodel-Rifle~\cite{stein-daniel-msc}. From the point of licensing, there is an important difference between \emph{using} the database \emph{via a network connection} and \emph{embedding} the database \emph{into software}. Since Neo4j's Community Edition, used by Codemodel-Rifle, is licensed under GPLv3~\cite{neo4j-licensing}, it can be used remotely via a network connection with practically any license because of the so-called \emph{application service provider loophole}~\cite{asp-loophole}, but it can not be embedded into applications which do not comply with GPLv3. As EPLv1 and GPLv3 are incompatible, Neo4j can not be embedded into the open-sourced Codemodel-Rifle.

Consequently, a necessary step was to switch from embedded Neo4j to remote Neo4j accessed via a driver. But, as native API-calls, which were extensively used by Codemodel-Rifle, can not be used with driver-accessed remote Neo4j, this caused further problems; these are subjects of the next subsections.


\subsection{Decomposing the Architecture}

Codemodel-Rifle's first architecture was monolith. It \emph{embedded} four key modules:

\begin{itemize}
\item a Neo4j graph \textbf{database},
\item a \textbf{webserver} exposing an HTTP REST API for interactions,
\item the \textbf{core module} responsible for transforming source code into an ASG,
\item and \textbf{other application logic}, e.g.\ for displaying and exporting AGSs into visual file formats like PDF or PNG.
\end{itemize}

Analysing the graph was possible either by running built-in Cypher queries via dedicated REST endpoints (e.g.\ \lstinline{/unusedfunctions}), or by submitting custom Cypher queries to the embedded database via the \lstinline{/run} endpoint.

Decoupling, or minimising direct interdependencies between components is an important aspect of software engineering. If a software is decomposed into smaller components along well-defined interfaces, it becomes modular: any module's inner functionality can be changed without affecting other modules, as long as the module implements the interface it was bound to. Motivations to alter a module include performance issues, scalability efforts, or changed domain logic. Codemodel-Rifle's first architecture was well-designed for easy manual testing and seemed to be an obvious solution for creating a small-scale analysis software. But several reasons required the framework to become modular to adapt.


\subsubsection{Detaching the Database}

Apart from the licensing issues detailed above, using a remote Neo4j server as a database instead of the embedded version comes with several benefits. The database can be outsourced onto a separate hardware or infrastructure: since analyses and graph maintenance can be demanding over large code repositories, providing dedicated resources for the database is an obvious solution for possible performance issues and scalability.

With a remote Neo4j database, a custom database driver can be utilised. This driver can be capable of incremental processing on the graph database level.\footnote{Gábor Szárnyas et al. are developing a graph database driver named \emph{ingraph} with the goal of evaluating openCypher queries incrementally~\cite{ingraph-github}.} Or it can provide an impermanent, in-memory local database instance for testing and for developing new analyses — to eliminate the need of installing a complete database server when long-term persistency is not explicitly needed.\footnote{Currently, the default configuration of the framework is to use an impermanent, in-memory graph database accessed via a Neo4j-compatible database driver.}

As a result of the aforementioned benefits and licensing issues, the framework was refactored to use a remote Neo4j server via a driver. This meant native API-calls were no longer possible: interacting with the database has been restricted to Cypher queries provided via the database driver. The Codemodel-Rifle framework extensively used native API-calls, so all these function calls had to be rewritten into distinct Cypher queries. As Cypher queries turned out to be notably slower than the API, when executed many queries at once, this introduced \emph{performance issues}. Solutions to these issues are described in the next sections.


\subsubsection{Eliminating the Web Interface}

The framework contained an embedded Grizzly~\cite{grizzly-website} web server to expose an HTTP REST API for user interactions. This was a convenient way for manual testing and a sensible approach for operating the software in a prospective production environment as well. All communication with the Codemodel-Rifle framework (operating as a server) could be achieved via its HTTP REST API with tool like curl~\cite{curl-website} or Postman~\cite{postman-website} (in development), or with an IDE or CI plugin (in production).

For automated testing, however, an HTTP REST API is inconvenient: solving important testing issues like exception handling are not straightforward. Since the framework is not yet ready for production use at all, but is heavily under development, an architectural decision was to eliminate the web server, and focus on the core functionality: the analyses. After removing the webserver from the architecture, the in-development way to supply code repositories to the framework for analysis is via unit tests: each test has its resources shipped along with the framework's source code.


\subsubsection{Separating the Visualisation Logic into an Isolated Project}

Visualising the ASG of an imported JavaScript source code is key to get familiar with Codemodel-Rifle's ASG-semantics, as well as for developing new analyses. \Cref{fig:codemodel-rifle-asg} displays an example of an ASG created and visualised by Codemodel-Rifle. However, the framework does not explicitly need this feature to perform analyses. Therefore it was a rational step to separate the visualisation logic into an isolated project, which is called \emph{Codemodel-Visualization}.


\subsection{Optimising for Testing Purposes}

The framework used embedded Neo4j as its storage: the project's folder contained a directory named \lstinline{database}, in which the full Neo4j embedded graph database was stored. Being embedded, the database instance was managed entirely by Codemodel-Rifle. After refactoring the framework to use an external Neo4j database server accessed via a driver because of the aforementioned licensing issues, testing became difficult. The following database-related steps were needed to run unit tests:

\begin{itemize}
\item the Neo4j Community Edition server software needed to be downloaded,
\item the designated directory to hold the database data needed to be selected,
\item the Neo4j server software needed to be started,
\item after the tests, the server needed to be stopped,
\item the database needed to be flushed after each test to ensure the necessary level of independence amongst the test cases.
\end{itemize}

This process can be partially automated with scripts, but it is still not a clean way to perform automated unit tests of Codemodel-Rifle.

As a solution, Gábor Szárnyas advised to use his \emph{neo4j-drivers} project~\cite{neo4j-drivers}. The package contains wrappers for the Neo4j Java driver: the \lstinline{EmbeddedTestkitDriver} makes possible to use a local, in-memory \lstinline{ImpermanentGraphDatabase} accessed via a driver. Using an impermanent, local database is convenient for use cases where persistency is not explicitly needed — e.g.\ testing and developing new analyses —, since no external Neo4j database needs to be installed and run. At the same time, Codemodel-Rifle can be easily reconfigured for production environments, where the framework needs to persist its data in an actual remote database. This reconfiguration only involves changing the framework's database driver in the \lstinline{DbServicesManager} class to another Neo4j-compatible one.


\subsection[Solutions to Speed-Related Issues]{Solutions to Speed-Related Issues: Object-Graph Mapping and the Cypher Query Builder}

Converting from embedded Neo4j to external, driver-accessed Neo4j, involving converting from \emph{persistent driver-accessed Neo4j} to \emph{in-memory driver-accessed Neo4j} introduced notable slowness, making testing and developing new analyses inconvenient again. \Cref{table:embedded-vs-in-memory-remote-table} compares the duration of visualising a simple JavaScript program (the runnning example's \lstinline{exporter} module seen on \Cref{fig:running-example-exporter}) with the old embedded, and the new in-memory driver-accessed approach.

\vspace*{-1mm}
\begin{table}[!htb]
	\centering
	\begin{tabular}{l|cc}
		\toprule
																								& \shortstack{\textbf{embedded} \\ \textbf{database}}
																								& \shortstack{\textbf{in-memory} \\ \textbf{driver-accessed database}}
																								\\
		\midrule
		\textbf{importing, transforming, storing}   &   82~ms      &   14,816~ms   \\
		\textbf{visualization}                      &   1,832~ms   &   2,456~ms    \\
		\midrule
		\textbf{total}                              &   1,914~ms   &   17,272~ms   \\
		\bottomrule
	\end{tabular}

	\caption[Speed comparison between the two database approach]{Speed comparison between the two database approach\footnotemark}
	\label{table:embedded-vs-in-memory-remote-table}
\end{table}

Seeing measurement results in \Cref{table:embedded-vs-in-memory-remote-table}\footnotetext{These measurements are only for demonstrating that the framework was so slow after the necessary refactorings that it needed to be optimised even for testing. They are not aimed to be fully accurate and complete. Evaluating the framework's performance with accurate measurements is the subject of \Cref{chapter:evaluation}.}, it was necessary to optimise the framework's performance for the in-memory driver-accessed database scenario, because extensive testing would not have been possible with such slowness. Apart from testing, optimisations benefit the in-production performance as well, since the testing and the production environments share the same interface: in both scenario, the database is accessed via a Neo4j driver. Ideally, the optimisations should be configurable to adapt to both the testing and the production environment. In the following paragraphs, I will summarise the optimisations I performed on the Codemodel-Rifle framework.

In Dániel Stein's implementation~\cite{stein-daniel-msc}, translating the ASG model to the property graph model happens simultaneously with actually storing the property graph model in the database. If an element of the ASG model has been successfully translated into the property graph model, it is stored in the database immediately. This can be optimised: by creating a property graph model stored in Java objects, and then implementing a storage logic to perform saving the objects into the database, the operative parameters of the storage logic can be optimised directly to the currently used database driver.

\subsubsection{Creating a specialised Object-Graph Mapping (OGM) Layer}

Importing a repository can be summarised by two types of database-level action.

\begin{enumerate}
\item \emph{Creating nodes} — the property graph model's nodes get created in Neo4j.
\item \emph{Setting relationships} — the property graph model's relationships get set in Neo4j.
\end{enumerate}

Therefore, a mapping layer basically needs to translate two object types: \emph{nodes} and \emph{relationships}. I mapped these two object types with the \lstinline{AsgNode} and \lstinline{AsgRelation} Java classes. An \lstinline{AsgNode} stores its properties in a \lstinline{HashMap}, and its labels and relations in two separate \lstinline{List} members. An \lstinline{AsgRelation} has a \lstinline{fromNode}, a \lstinline{toNode}, and a \lstinline{relationshipLabel} member. Storing relationship properties was omitted, since the Codemodel-Rifle framework semantics does not contain relationship properties.

Identifying nodes is achieved with a universally unique identifier (UUID), instead of the earlier approach of using Neo4j's discouraged \lstinline{id()} function to get the nodes' built-in identifier. Each \lstinline{AsgNode} object has an \lstinline{id} member, which contains a value generated using the \lstinline{java.util.UUID} package. The \lstinline{id} member gets automatically translated into the property graph as well as all other properties. With a mapping layer like the above, it is possible to customise the procedure of storing the model in the database e.g.\ by optimising query granularity.


\subsubsection{The Cypher Query Builder}

A main bottleneck identified with the \lstinline{ImpermanentGraphDatabase} instance of the \lstinline{EmbeddedTestkitDriver} interface was the speed of parsing queries. The example presented in \Cref{table:embedded-vs-in-memory-remote-table} requires $201$ property graph nodes and $340$ relationships to be created, so it normally requires $541$ distinct Cypher queries to be run. As per my experience of manually performed testing, if several distinct queries are merged into one, it increases speed significantly. Accordingly, it was a reasonable step to implement a configurable, specialised query builder, which manages storing the property graph model with a coarser query-granularity (by creating more than one nodes or setting more than one relationships within one executed database query).

The query builder I implemented is capable of creating Cypher queries specially for the aforementioned OGM layer, following its internal configuration of how many \emph{node creator} queries and how many \emph{relation setter} queries should be merged (compressed) into one. The builder assembles and prepares the queries, and then returns them in a list. Each database query in the list is ready to be executed without further modifications.


\subsubsection{Refactoring the Core Logic to Utilise the OGM and the Query Builder}

After implementing and testing the mapping layer and the query builder, I modified the core import logic of the framework in the \lstinline{ASTScopeProcessor} class to utilise the new components. Instead of immediately storing the translated ASG model as a property graph model in the database, the processor first stores the property graph model in Java objects with my custom OGM layer. Then, benefiting from the query builder, the model is sent to the database in optimally sized chunks following the query builder's configuration.

\Cref{table:query-builder-config} shows the optimal configuration values of the query builder in testing environment (with the \lstinline{EmbeddedTestkitDriver}), and in a prospective production environment (with the official Neo4j driver) for test cases run on my computer.

\begin{table}[!htb]
	\centering
	\begin{tabular}{l|cc}
		\toprule
																								&   \textbf{testing}   &   \textbf{production}   \\
		\midrule
		\textbf{nodes created in one query}         &   16                 &   20                    \\
		\textbf{relationships set in one query}     &   1                  &   2                     \\
		\bottomrule
	\end{tabular}

	\caption{Optimal configuration of the query builder for my computer}
	\label{table:query-builder-config}
\end{table}


\subsubsection{Results of Speed-Related Refactorings}

\Cref{table:results-of-query-optimisations} shows a comparison between the speed of two versions of the framework when importing the \lstinline{exporter} module of the running example, presented in \Cref{fig:running-example-exporter}.\footnotetext{These measurements are only for demonstrating that the framework became notably faster after the speed-related refactorings. They are not aimed to be fully accurate and complete. Evaluating the framework's performance with accurate measurements is the subject of \Cref{chapter:evaluation}.} Both versions presented here use the in-memory driver-accessed database, but the first does not use the optimisations implemented (the mapping layer and the query builder), while the second one does.

\begin{table}[!htb]
	\centering
	\begin{tabular}{l|cc}
		\toprule
																								&   \textbf{without optimisations}     &   \textbf{with optimisations}   \\
		\midrule
		\textbf{importing, transforming, storing}   &   14,816~ms                          &   7,031~ms                      \\
		\textbf{visualization}                      &   2,456~ms                           &   2,432~ms                      \\
		\midrule
		\textbf{total}                              &   17,272~ms                          &   9,463~ms                      \\
		\bottomrule
	\end{tabular}

	\caption[Speed comparison with and without optimisations]{Speed comparison with and without optimisations\footnotemark}
	\label{table:results-of-query-optimisations}
\end{table}


\subsection{Other Performances}

After the refactoring, the framework's package structure got very complex. Several main features of the software — like source code parsing and other actions to be exposed onto the external interface for user interactions — were mixed with internal operations like database management and utilities. I separated the packages this way: \lstinline{actions} contains features to be exposed to the user, \lstinline{database} contains database-related operations, \lstinline{tasks} contains internal features not to be exposed, and \lstinline{utils} contains utilities.

The final version of Dániel Stein's framework used the \lstinline{v2.2.0} version of Shape Security's Shift parser and scope analyser. This version only supports the 6\textsuperscript{th} Version of \es. Since then, version \lstinline{es2016-v1.1.1} supporting the full ES7 specification was released by Shape Security~\cite{shift-ast, shift-java-github}. I updated the framework's dependencies to use the new version of the parser and scope analyser.

\subsection{Summary of Refactoring}

\Cref{fig:codemodel-rifle-refactored-architecture} presents a high-level overview of the refactored architecture of the Codemodel-Rifle framework. Besides becoming modular, the framework has gone through a series of optimisations to simplify testing and developing new analyses.

\begin{figure}[!htb]
	\centering
	\includegraphics[height=\textheight-37mm,clip]{figures/codemodel-rifle-refactored-architecture.pdf}
	\caption{The new architecture of Codemodel-Rifle with my contributions \emph{emphasised}}
	\label{fig:codemodel-rifle-refactored-architecture}
\end{figure}


\newpage
\section{In Development: Steps of Building New Analyses}
\label{section:steps-of-building-new-analyses}

Building new analyses for software defects basically consists of three steps. The steps are detailed in the following subsections.


\subsection{Visualising the Defect with Codemodel-Visualisation}

Without seeing what to search for, new analyses can not be implemented. A defect's signature has to be inspected with Codemodel-Rifle's semantics first. For visualising a defect pattern, a new unit test has to be created in the Codemodel-Visualization project. The JavaScript modules containing the defect should be included as test resources.

Using Codemodel-Rifle as a dependency, Codemodel-Visualization first parses the JavaScript files given as test resources and translates them to separate property graphs. If more than one source files were imported, their graphs are interconnected along the export and import semantics of \es.\footnote{The process of interconnecting \es modules along export and import statements is one of the key subjects of this thesis. It will be detailed in \Cref{chapter:elaboration}.} Finally, the full property graph model gets exported into a visual file format, like PDF or PNG. The export format is configurable in the unit test.


\subsection{Describing the Defect Pattern}

The file exported by Codemodel-Visualization precisely mirrors the property graph instance model translated by Codemodel-Rifle, but some nodes and edges are not displayed to preserve the transparency of the visualised graph.\footnote{Ignored nodes and edges are listed in the \lstinline{GraphWalker} class as filtered entities from the underlying visitor pattern implementation. As an earlier architectural decision, this is not configurable externally.} Any pattern seen in the visualised graph can be directly matched by Codemodel-Rifle.


\subsection{Implementing the Analysis}
\label{subsection:implementing-analyses}

Analyses are basically Cypher queries. If a defect's pattern can be expressed with a Cypher query, it can be detected by the framework.

Some defects are more high-level or more general than to present their patterns in an intact graph directly. Detecting complex errors like these may require to extensively manipulate the graph to dredge defect patterns for matching. In cases involving \emph{transitive} defects, like in the running example\footnote{The running example is to detect a division by zero scenario. But zero is not a numeric literal $0$, but the indirectly referenced return value of a nested function stack with variable assignments and also a module boundary in between.} presented in \Cref{chapter:background}, a flag like \lstinline{EqualsZero} has to be propagated through the graph along specified edges: variable assignments, variable references, function call and function return statements, etc.

Transitive graph manipulations can be achieved by introducing \emph{qualifiers} into the analysis. The concept of qualifiers will be described in detail in \Cref{chapter:elaboration}.

If an analysis matches the specified pattern, it returns the following:

\begin{itemize}
\item a \textbf{message} to explain the type of the defect for a human reader,
\item an \textbf{entity name} (or an empty string) to identify defects bound to named entities like variables and functions,
\item the \textbf{path} of the containing module,
\item the \textbf{line} in which the defect was found,
\item the \textbf{column} of the line at which the defect begins.
\end{itemize}

In my current implementation, the above items are uniformly\footnote{Exactly these items are returned in all cases, regardless of the defect's type. This is not flexible, since an \emph{unreachable code} defect may require other arguments to be logged, than a \emph{non-initialised variable} defect.} returned from the database as elements of a Neo4j \lstinline{Record}, and they are handed over to a central logger to be immediately printed after minimal formatting. This is not a flexible solution; in the future, this basic defect processing logic should be refined. The found defects could be returned as JSON objects from the database to be easily parsed into a Java class named \lstinline{Defect}. They could also be collected into a per-analysis data structure. This way, the framework could display defects found at an analysis according to various aspects and criteria, and it could also produce machine-readable output. With a clean API, this would allow the framework to be embedded into other software.


\section{In Production: Steps of Operating Live}

The prospective live operation of the framework basically consists of three steps, which are managed by the framework. Ideally, the operation should be automatic and transparent: if a change is done in the IDE, or a new commit is pushed to the central repository, the framework should perform analyses over the changed code repository. The steps of a full analysis procedure are detailed in the following subsections.


\subsection{Import: Synchronising the Repository into the Framework}

First, the code repository is imported into the framework. This involves listing and parsing all files with configured extensions (currently only \lstinline{.js}), then saving the created property graph models into the database.

The word synchronising expresses that Codemodel-Rifle aims to be incremental; but while it does so, its capabilities are still very limited. According to plans, the framework will cooperate with VCSs to detect changes, thus it will be able to import only those files that changed since the last import process.


\subsection{Interconnect: Connecting the Related \es Modules}

To evaluate analyses over more than one \es modules, the related modules' separate property graphs are interconnected along the export and import semantics of \es. This process is described in detail in \Cref{chapter:elaboration}.


\subsection{Analyse: Performing Analyses}

Performing analyses can be broken down into two substeps.


\subsubsection{Manipulating the Graph}

Complex analyses may require to extensively manipulate the graph. These manipulations involving qualifiers are processed first.


\subsubsection{Querying the Graph}

The graph is queried with Cypher, with matching predefined graph patterns developed with the aforementioned steps. If a defect pattern matches, it gets logged onto the console with the semantics described in \Cref{subsection:implementing-analyses}, in the format seen in \Cref{fig:defect-found-logger}.

\begin{figure}[!htb]
	\centering
	\begin{lstlisting}[language=Rifle]
				message: entityname at line:column in path
	\end{lstlisting}
  \caption{The framework's console output if a defect was found}
  \label{fig:defect-found-logger}
\end{figure}