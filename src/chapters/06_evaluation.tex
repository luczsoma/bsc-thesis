\chapter{Evaluation of Performance}
\label{chapter:evaluation}

In this chapter, I evaluate the framework's performance by measuring the duration of analysing several source code repositories.


\section{Evaluation Environment}

\subsection{Computer Configuration}

The measurements were performed on my computer for the sake of simplicity. During a measurement session, my computer was plugged in, it was configured to utilise its full performance, and only those software were running, which were explicitly necessary for the measurements. Each measurement was preceded by a full system reboot.

The major points of the my computer's configuration is the following:

\begin{itemize}
\item Apple MacBook Pro, Mid-2014, 13 inches;
\item Intel Core i5, 2.6 GHz;
\item 8 GB 1600 MHz DDR3 RAM;
\item 250 GB SSD.
\end{itemize}


\subsection{Software Configuration}

As currently the Codemodel-Rifle framework does not have any interface to interact with, the measurements were performed as per-repository unit tests with logging test results onto the console.

\begin{itemize}
\item \textbf{Runtime:} JetBrains IntelliJ IDEA Ultimate 2016.3.4
\item \textbf{Java Runtime Environment:} 1.8.0\_112-release-408-b6 x86\_64
\item \textbf{Java Virtual Machine:} OpenJDK 64-Bit Server VM by JetBrains s.r.o (initial memory allocation pool: 4 GB, maximum memory allocation pool: 8 GB)
\item \textbf{Database:} Neo4j Community Edition 3.1.3 Server (initial heap size: 4 GB, maximum heap size: 8 GB, page cache size: 8 GB, transaction log retention policy: 1 day)
\item \textbf{Database driver:} Neo4j Bolt driver for Java 1.1.1
\end{itemize}

For better performance, a database index was defined in Neo4j for the \lstinline{`id'} property of all nodes labeled with \lstinline{'AsgNode'} (practically all nodes created by Codemodel-Rifle).


\section{Measurement Goals and Methods}

\subsection{Selection Criteria of the Analysed Source Code Repositories}

The evaluation was performed on popular open-source JavaScript code repositories found on GitHub. The 40 randomly chosen repositories differ in size, in the number of lines of code, and in the number of distinct source files. The repositories are listed in the Appendix.


\subsection{Key Performance Indices}

The goal of the performance evaluation was to determine the time characteristics of the refactored Codemodel-Rifle framework, and especially the implemented analyses. Based on the production operation of the framework detailed in the introduction of \Cref{chapter:elaboration}, the following four Key Performance Indices have been determined to be measured.

\begin{itemize}
\item \textbf{The duration of synchronisation:} the time period between starting the importing process of a code repository (excl. finding all \lstinline{.js} files, and reading the contents of the source files) and saving the last module's last \lstinline{AsgNode} into the database.
\item \textbf{The duration of interconnection:} this time period encompasses searching for semantically valid interconnections between modules, and actually performing the interconnections.
\item \textbf{The duration of running the Qualifier System:} this time period encompasses initialising the Qualifier System, and propagating the qualifiers.
\item \textbf{The duration of performing the analyses:} this time period encompasses trying to match all predefined analysis patterns.
\item \textbf{The total duration of the analysis process:} this time period is the sum of the above four durations. It was calculated, not measured.
\end{itemize}

Besides the Key Performance Indices, the number of nodes and relationships created during the synchronisation of a repository is also recorded.


\subsection{Process of Measurement}

All analysed code repositories were measured four times in a session in order to avoid biases caused by the environment. Each session was preceded by a full system restart. The final measurement results of a repository are averaged from the four different values.


\section{Measurement Results}

In this section, I present and evaluate the measurement results of the aforementioned Key Performance Indices.



\subsection{Synchronisation}

At first, the repository needs to be synchronised into Codemodel-Rifle. At this time, the source code files of the repository gets translated to distinct, per-module property graphs.

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-nodes-relationships-sloc.pdf}
	\caption{The characteristics of synchronising repositories into Codemodel-Rifle}
	\label{fig:measurement-nodes-relationships-sloc}
\end{figure}

There are many coding styles and conventions, and the contents of the source files can vary from per-line exported configuration constants to program codes without physical line breaks. Nevertheless, there is a linear relationship between the number of code lines and the number of created ASG nodes and relationships in the analysed repositories.

\Cref{fig:measurement-nodes-relationships-sloc} presents the correlation of the source lines of code (SLOC) and the number of ASG nodes and relationships created during synchronising the code bases into Codemodel-Rifle. In terms of SLOC, the smallest repository imported was \lstinline{initialstate/silent-doorbell} 15 lines of code (686 nodes and 2306 relationships created), while the largest was \lstinline{tresorit/webclient} with 34,546 lines of code (with 1,346,776 nodes and 4,576,319 relationships).


\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-synctime-sloc.pdf}
	\caption{The characteristics of synchronising repositories into Codemodel-Rifle}
	\label{fig:measurement-synctime-sloc}
\end{figure}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-synctime-nodes-relationships.pdf}
	\caption{Synchronising repositories into Codemodel-Rifle}
	\label{fig:measurement-synctime-nodes-relationships}
\end{figure}


\subsection{Interconnection}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-interconnecttime-modules.pdf}
	\caption{The characteristics of interconnecting related modules}
	\label{fig:measurement-interconnecttime-modules}
\end{figure}


\subsection{The Qualifier System}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-qualifiersystem-nodes-relationships.pdf}
	\caption{The characteristics of running the Qualifier System}
	\label{fig:measurement-qualifiersystem-nodes-relationships}
\end{figure}


\subsection{Analysis}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-analysis-nodes-relationships.pdf}
	\caption{The characteristics of performing the analyses}
	\label{fig:measurement-analysis-nodes-relationships}
\end{figure}


\subsection{Total Duration of the Analysis Process}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-totaltime-sloc.pdf}
	\caption{The characteristics of performing the analyses}
	\label{fig:measurement-totaltime-sloc}
\end{figure}

\begin{figure}[!htb]
	\centerfloat
	\includegraphics[width=\textwidth,clip]{figures/measurement-totaltime-nodes-relationships.pdf}
	\caption{The characteristics of performing the analyses}
	\label{fig:measurement-totaltime-nodes-relationships}
\end{figure}


\section{Defects Found by the Framework}

The framework found only two types of defects in the 40 analysed repositories: 897 cases of uninitialised variables, and 134 cases of globally unused exports were found. As the analysis is neither sound, nor complete, these numbers can be inaccurate. However, I inspected a randomly chosen subset of the found defects manually, and — according to my experience — the defects were indeed present in all cases.


\section{Threats to Validity}

I designed the measurements to be as accurate and complete as possible. Nevertheless, there are factors which I could not fully control, and these may influence the results. In this section, I summarise the factors which could bias the measurements.


\paragraph{Measurements on a Consumer Laptop}
Since my computer runs an operating system targeted for consumer usage, it may contain software running in the background, which influence measurement factors like processor or memory usage. I tried to mitigate these effects by configuring the computer to utilise all resources for the measurement procedure, by running the measurement processes multiple times, and by analysing a larger number of code repositories independently.


\paragraph{Graph Query Optimisations}
I tried to optimise the graph queries of the interconnections and the analyses as much as I could. However, since I am not an expert in the internals of Cypher queries, it is possible that some queries can be optimised further. Therefore, the characteristics of the interconnections or the analyses may not be fully correct.


\paragraph{Methological Mistakes}
It is possible that I made other methodological mistakes at implementing the analyses or the measurements. Using a fluid, internal semantics for the interconnection of modules incorrectly can be an example of a such mistake.